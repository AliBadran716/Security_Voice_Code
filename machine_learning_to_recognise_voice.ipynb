{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.440191900Z",
     "start_time": "2024-01-03T22:23:19.787645500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01476d2d0c39c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.442191300Z",
     "start_time": "2024-01-03T22:23:23.432177500Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_mfccs(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mfccs = mfccs.T\n",
    "    delta_mfccs = librosa.feature.delta(mfccs)\n",
    "    delta_mfccs = delta_mfccs\n",
    "    \n",
    "    #(216) rows = frames ,  and (26) columns = features\n",
    "    features = np.hstack([mfccs, delta_mfccs])\n",
    "    feature_names = [f\"MFCC_{i+1}\" for i in range(features.shape[1])]\n",
    "    return pd.DataFrame(features, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d0d2cc5bd13977",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.457721600Z",
     "start_time": "2024-01-03T22:23:23.439193200Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_pitch(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
    "    pitch = np.mean(pitches[magnitudes > np.max(magnitudes) * 0.85])\n",
    "    return pd.DataFrame({\"Pitch\": [pitch]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf1083d8ba507ce",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.460075Z",
     "start_time": "2024-01-03T22:23:23.453209800Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_chroma(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma = chroma.T\n",
    "    feature_names = [f\"Chroma_{i+1}\" for i in range(chroma.shape[1])]\n",
    "    return pd.DataFrame(chroma, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbce45c9fa59891",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.498064300Z",
     "start_time": "2024-01-03T22:23:23.458721800Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_zero_crossings(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    zero_crossings = librosa.feature.zero_crossing_rate(y)\n",
    "    zero_crossings = zero_crossings.T\n",
    "    feature_names = [f\"ZeroCrossings_{i+1}\" for i in range(zero_crossings.shape[1])]\n",
    "    return pd.DataFrame(zero_crossings, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd08dc1bc04c8ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.500069500Z",
     "start_time": "2024-01-03T22:23:23.465592Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_spectral_contrast(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spectral_contrast = spectral_contrast.T \n",
    "    feature_names = [f\"SpectralContrast_{i+1}\" for i in range(spectral_contrast.shape[1])]\n",
    "    return pd.DataFrame(spectral_contrast, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ee10899d914629",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:23.501067200Z",
     "start_time": "2024-01-03T22:23:23.475309800Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_user_folder(folder_path):\n",
    "    mfccs_list = []\n",
    "    pitch_list = []\n",
    "    chroma_list = []\n",
    "    zero_crossings_list = []\n",
    "    spectral_contrast_list = []\n",
    "    flag = 1\n",
    "\n",
    "    # Iterate over all .wav files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if flag:\n",
    "                mfccs_list = extract_mfccs(file_path)\n",
    "                pitch = extract_pitch \n",
    "                pitch_list.append(pitch)\n",
    "                chroma_list = extract_chroma(file_path)\n",
    "                zero_crossings_list = extract_zero_crossings(file_path)\n",
    "                spectral_contrast_list = extract_spectral_contrast(file_path)\n",
    "\n",
    "                flag = 0\n",
    "            else:\n",
    "                # Extract features for each file\n",
    "                mfccs = extract_mfccs(file_path)\n",
    "                pitch = extract_pitch(file_path)\n",
    "                chroma = extract_chroma(file_path)\n",
    "                zero_crossings = extract_zero_crossings(file_path)\n",
    "                spectral_contrast = extract_spectral_contrast(file_path)\n",
    "                # Append features to the lists\n",
    "                mfccs_list = pd.concat([mfccs,mfccs_list]) \n",
    "                chroma_list = pd.concat([chroma, chroma_list])\n",
    "                zero_crossings_list = pd.concat([zero_crossings, zero_crossings_list])\n",
    "                spectral_contrast_list = pd.concat([spectral_contrast, spectral_contrast_list])\n",
    "                pitch_list.append(pitch)\n",
    "\n",
    "\n",
    "    # Create feature matrix with separate columns for each feature type\n",
    "    #Data set without the pitch\n",
    "    compined_features = pd.concat([mfccs_list, chroma_list, zero_crossings_list, spectral_contrast_list], axis=1)\n",
    "\n",
    "    return compined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc41ef27254ba4e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:49.316385400Z",
     "start_time": "2024-01-03T22:23:23.482061400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdulla_ahmed\n",
      "ahmed_ali\n",
      "ashf\n",
      "atef\n",
      "bedro\n",
      "grant_me_access\n",
      "hassan\n",
      "hazem_rafaat\n",
      "muhannad\n",
      "open_middle_door\n",
      "unlock_the_gate\n",
      "Voices\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the main folder\n",
    "main_folder_path = \"voices\"\n",
    "\n",
    "# Get a list of all subfolders inside the main folder\n",
    "subfolders = [f.path for f in os.scandir(main_folder_path) if f.is_dir()]\n",
    "# create a list of two train datas one for voice recognition (ahmed_ali, bedro, hassan, muhannad) and the other for speech recognition (grant_me_access,unlock_the_gate,open_middle_door)\n",
    "data_voice_recognition_features = {\n",
    "    \"ahmed_ali\": [],\n",
    "    \"bedro\": [],\n",
    "    \"hassan\": [],\n",
    "    \"muhannad\": []\n",
    "\n",
    "}\n",
    "data_speech_recognition_features = {\n",
    "    \"grant_me_access\": [],\n",
    "    \"open_middle_door\": [],\n",
    "    \"unlock_the_gate\": []\n",
    "}\n",
    "\n",
    "# Loop over each subfolder\n",
    "for subfolder_path , i in zip(subfolders , range(len(subfolders))) :\n",
    "    # Extract the name of the subfolder\n",
    "    subfolder_name = os.path.basename(subfolder_path)\n",
    "    print(subfolder_name)\n",
    "    \n",
    "    # Create a variable with the features of the subfolder\n",
    "    features = process_user_folder(subfolder_path)\n",
    "\n",
    "    # Assign the variable name based on the subfolder name\n",
    "    globals()[subfolder_name] = features\n",
    "    if subfolder_name == \"ahmed_ali\" or subfolder_name == \"bedro\" or subfolder_name == \"hassan\" or subfolder_name == \"muhannad\" or subfolder_name == \"abdulla_ahmed\" or subfolder_name == \"ashf\" or subfolder_name == \"atef\" or subfolder_name == \"hazem_rafaat\":\n",
    "        data_voice_recognition_features[subfolder_name] = features\n",
    "    else:\n",
    "        data_speech_recognition_features[subfolder_name] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66834a6d90538c5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:23:49.340458100Z",
     "start_time": "2024-01-03T22:23:49.320387100Z"
    }
   },
   "outputs": [],
   "source": [
    "# do the same for voice recognition\n",
    "# Combine all the extracted features into a single DataFrame\n",
    "all_features = pd.concat([data_voice_recognition_features[\"ahmed_ali\"], data_voice_recognition_features[\"bedro\"], data_voice_recognition_features[\"hassan\"], data_voice_recognition_features[\"muhannad\"]])\n",
    "# Create labels for the features based on the subfolder names\n",
    "labels = [\"ahmed_ali\"] * data_voice_recognition_features[\"ahmed_ali\"].shape[0] + [\"bedro\"] * data_voice_recognition_features[\"bedro\"].shape[0] + [\"hassan\"] * data_voice_recognition_features[\"hassan\"].shape[0] + [\"muhannad\"] * data_voice_recognition_features[\"muhannad\"].shape[0] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4925569729d17462",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T22:24:22.820145900Z",
     "start_time": "2024-01-03T22:23:49.341459700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.90\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   ahmed_ali       0.92      0.93      0.92       482\n",
      "       bedro       0.96      0.90      0.93       526\n",
      "      hassan       0.83      0.91      0.87       571\n",
      "    muhannad       0.90      0.82      0.86       379\n",
      "\n",
      "    accuracy                           0.90      1958\n",
      "   macro avg       0.90      0.89      0.89      1958\n",
      "weighted avg       0.90      0.90      0.90      1958\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=300, random_state=15, min_samples_split=2, min_samples_leaf=1, max_depth=50)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_rep_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n",
    "print('Random Forest Classification Report:\\n', classification_rep_rf)\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(rf_model, model_file)\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1ba84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T22:24:23.066811700Z",
     "start_time": "2024-01-03T22:24:22.821147600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming your model is stored in 'rf_model' and scaler is stored in 'scaler'\n",
    "with open('rf_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_model, model_file)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed292696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T22:24:23.071480100Z",
     "start_time": "2024-01-03T22:24:23.067811500Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    # Extract features for the input file\n",
    "    mfccs = extract_mfccs(file_path)\n",
    "    pitch = extract_pitch(file_path)\n",
    "    chroma = extract_chroma(file_path)\n",
    "    zero_crossings = extract_zero_crossings(file_path)\n",
    "    spectral_contrast = extract_spectral_contrast(file_path)\n",
    "\n",
    "    # Combine the features into a single DataFrame\n",
    "    features = pd.concat([mfccs, chroma, zero_crossings, spectral_contrast], axis=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca7d04cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T22:41:58.293932600Z",
     "start_time": "2024-01-03T22:41:58.221310400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['ahmed_ali', 'bedro', 'hassan', 'muhannad'], dtype='<U9'), array([54,  2, 51,  1], dtype=int64))\n",
      "ahmed_ali\n"
     ]
    }
   ],
   "source": [
    "def predict_person(file_path):\n",
    "        # Extract features from the input file\n",
    "        features = process_file(file_path)\n",
    "\n",
    "        # Scale the features using the loaded scaler\n",
    "        input_features_scaled = scaler.transform(features)\n",
    "\n",
    "        # Make predictions using the loaded model\n",
    "        prediction = rf_model.predict(input_features_scaled)\n",
    "        prediction = np.unique(prediction, return_counts=True)\n",
    "        print(prediction)\n",
    "        return prediction[0][np.argmax(prediction[1])]\n",
    "\n",
    "print(predict_person(\"honda_trial_2.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e470bc7448ad9a34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
